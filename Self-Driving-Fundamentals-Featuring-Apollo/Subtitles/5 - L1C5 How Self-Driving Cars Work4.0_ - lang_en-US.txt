1
00:00:09,080 --> 00:00:10,320
Hello.

2
00:00:10,320 --> 00:00:11,470
Hello.

3
00:00:12,760 --> 00:00:17,840
I teach the Self-Driving Car Engineer Nanodegree Program at Udacity,

4
00:00:17,840 --> 00:00:20,200
which is a nine-month program that trains

5
00:00:20,200 --> 00:00:23,170
software engineers to work on autonomous vehicles.

6
00:00:23,170 --> 00:00:28,630
Before I was ever an employee at Udacity, I was actually a student.

7
00:00:28,630 --> 00:00:33,490
I was for many years a normal Silicon Valley Web Software Engineer working in Ruby on Rails 

8
00:00:33,490 --> 00:00:38,370
and a few years ago I got really excited about self-driving cars,

9
00:00:38,370 --> 00:00:44,340
but I didn't have any background in robotics or system software or automotive software.

10
00:00:44,340 --> 00:00:47,505
So, I started taking classes online at Udacity and at

11
00:00:47,505 --> 00:00:51,375
other places to learn how to become an autonomous vehicle engineer.

12
00:00:51,375 --> 00:00:54,360
I was hired on to the autonomous vehicle team at

13
00:00:54,360 --> 00:00:58,380
Ford Motor Company at their Research and Innovation Center in Palo Alto,

14
00:00:58,380 --> 00:01:03,420
California and later the team at Udacity asked me to come and build

15
00:01:03,420 --> 00:01:06,420
out a program for people to do essentially what I

16
00:01:06,420 --> 00:01:10,550
had done to learn to become autonomous vehicle engineers.

17
00:01:10,980 --> 00:01:14,925
At Udacity, we have our own self-driving car,

18
00:01:14,925 --> 00:01:16,830
her name is Karla,

19
00:01:16,830 --> 00:01:20,280
and I'd like to tell you a little bit about how

20
00:01:20,280 --> 00:01:23,250
Karla works and how most self-driving cars

21
00:01:23,250 --> 00:01:25,890
work through the lens of some of the projects that

22
00:01:25,890 --> 00:01:30,045
our students have done as part of the nanodegree program.

23
00:01:30,045 --> 00:01:35,925
So, I think of self-driving cars as having five core components.

24
00:01:35,925 --> 00:01:39,375
Computer Vision, Sensor Fusion,

25
00:01:39,375 --> 00:01:44,770
Localization, Path Planning, and Control.

26
00:01:44,770 --> 00:01:48,240
Computer vision is how we use camera images

27
00:01:48,240 --> 00:01:51,780
to figure out what the world around us looks like and

28
00:01:51,780 --> 00:01:55,530
sensor fusion is how we incorporate data from other sensors like

29
00:01:55,530 --> 00:02:00,405
lasers and radar to get a richer understanding of our environment.

30
00:02:00,405 --> 00:02:05,050
Once we built this deep understanding of what the world around us is,

31
00:02:05,050 --> 00:02:08,820
we use localization to figure out precisely where we are in

32
00:02:08,820 --> 00:02:11,520
that world and then once we figured

33
00:02:11,520 --> 00:02:14,790
out where we are in the world and what the world looks like,

34
00:02:14,790 --> 00:02:18,840
we use path planning to chart a course through the world to get us to

35
00:02:18,840 --> 00:02:22,955
where we'd like to go and then the final step is control,

36
00:02:22,955 --> 00:02:26,740
which is how we actually turn the steering wheel and hit the throttle and hit

37
00:02:26,740 --> 00:02:31,700
the brake in order to execute the trajectory that we built during path planning.

38
00:02:31,700 --> 00:02:34,330
So, these are the five core components of

39
00:02:34,330 --> 00:02:38,190
self-driving cars and I'd like to investigate each of them with you.

40
00:02:38,190 --> 00:02:40,375
This is computer vision,

41
00:02:40,375 --> 00:02:43,810
this is a video that Karla took driving up

42
00:02:43,810 --> 00:02:48,625
Highway 280 in Silicon Valley and this is work that one of our students, Vivek Adab, did.

43
00:02:48,625 --> 00:02:50,920
Vivek is using computer vision so looking for

44
00:02:50,920 --> 00:02:54,180
colors and edges and gradients to define the lane on

45
00:02:54,180 --> 00:02:56,310
the road and then he's trained

46
00:02:56,310 --> 00:03:00,750
a deep neural network to draw bounding boxes around the other vehicles on the road.

47
00:03:00,750 --> 00:03:02,820
Deep neural networks or deep learning is

48
00:03:02,820 --> 00:03:06,480
an exciting new part of machine learning in artificial intelligence and it's

49
00:03:06,480 --> 00:03:09,180
a way that computers can learn what cars and

50
00:03:09,180 --> 00:03:12,770
other objects look like simply by sending them lots and lots of data.

51
00:03:12,770 --> 00:03:15,690
They see lots of cars and they learn what cars look like and

52
00:03:15,690 --> 00:03:18,640
you can see the deep neural network is trying to figure out is that one car,

53
00:03:18,640 --> 00:03:20,545
is that two cars and this they separate,

54
00:03:20,545 --> 00:03:24,060
it sees that it's a couple of cars and this is pretty similar

55
00:03:24,060 --> 00:03:28,480
to what advanced driver assistance systems do on the road today.

56
00:03:28,840 --> 00:03:33,820
So, once we know what the world looks like through camera images,

57
00:03:33,820 --> 00:03:36,640
the next step is to augment that understanding of

58
00:03:36,640 --> 00:03:40,810
the world using other sensors so radar and lasers to get

59
00:03:40,810 --> 00:03:43,680
measurements that are difficult for a camera alone to

60
00:03:43,680 --> 00:03:47,070
understand so things like the distance between us and other cars and

61
00:03:47,070 --> 00:03:51,390
how fast other objects in the environment are moving and what you see

62
00:03:51,390 --> 00:03:55,980
here is data from a trip that Karla took up El Camino Real,

63
00:03:55,980 --> 00:04:00,660
which is the main commercial street in Silicon Valley and you see on the right,

64
00:04:00,660 --> 00:04:05,655
the video feed of what that drive looks like and the main image here is

65
00:04:05,655 --> 00:04:10,950
the laser scan of what the world around Karla looks like and you can see this LIDAR,

66
00:04:10,950 --> 00:04:14,580
which is an array of lasers doing a 360-degree scan of

67
00:04:14,580 --> 00:04:16,380
the world and seeing what

68
00:04:16,380 --> 00:04:20,200
the different objects in that environment look like and how they move.

69
00:04:20,680 --> 00:04:27,090
So, once we understand both what the world looks like and how to measure it and we

70
00:04:27,090 --> 00:04:29,190
incorporate those understandings together to get

71
00:04:29,190 --> 00:04:32,550
a rich picture of our surrounding environment,

72
00:04:32,550 --> 00:04:36,095
the next step is to localize ourselves in that environment.

73
00:04:36,095 --> 00:04:39,490
And you might think this is really straightforward because we all have cell phones today,

74
00:04:39,490 --> 00:04:41,025
they all have GPS in them,

75
00:04:41,025 --> 00:04:43,680
we all know where we are all of the time,

76
00:04:43,680 --> 00:04:46,200
but in fact GPS is only accurate to within

77
00:04:46,200 --> 00:04:51,270
about one to two meters and if you think about how big one to two meters is,

78
00:04:51,270 --> 00:04:53,700
if a car is wrong about where it is by

79
00:04:53,700 --> 00:04:57,935
one to two meters it could be over here on the sidewalk hitting things.

80
00:04:57,935 --> 00:05:00,640
That would be really bad. So, we have to use

81
00:05:00,640 --> 00:05:03,010
much more sophisticated mathematical algorithms as

82
00:05:03,010 --> 00:05:05,530
well as high definition maps to localize

83
00:05:05,530 --> 00:05:07,660
our vehicle precisely in its environment to

84
00:05:07,660 --> 00:05:13,250
single digit centimeter level accuracy so this is work by one of our students.

85
00:05:13,250 --> 00:05:16,580
His name is Daniel Lopez Madrid and what Daniel is doing is,

86
00:05:16,580 --> 00:05:18,960
he is using a particle filter,

87
00:05:18,960 --> 00:05:21,435
which is a really sophisticated type of

88
00:05:21,435 --> 00:05:25,050
triangulation and as his blue vehicle moves through the simulator,

89
00:05:25,050 --> 00:05:27,900
it's measuring its distance from various landmarks and

90
00:05:27,900 --> 00:05:30,810
it's figuring out how far it is from these landmarks and where it

91
00:05:30,810 --> 00:05:34,740
sees the landmarks and comparing that to the map and using that to figure

92
00:05:34,740 --> 00:05:39,010
out precisely where it is in the world and this is how self-driving cars localize.

93
00:05:39,010 --> 00:05:42,810
In the real world, those landmarks might be things like street lights

94
00:05:42,810 --> 00:05:48,070
or traffic signs or mailboxes or even manhole covers.

95
00:05:48,610 --> 00:05:51,910
So, once we figured out where we are in

96
00:05:51,910 --> 00:05:54,910
the world and what the world around us looks like,

97
00:05:54,910 --> 00:05:58,240
the next step is to actually chart a path through that world

98
00:05:58,240 --> 00:06:02,370
to figure out how to get where we want to go and this is path planning.

99
00:06:02,370 --> 00:06:06,235
Here, you can see the work by one of our students, Kazahiro Nakagawa,

100
00:06:06,235 --> 00:06:10,620
and what Kazahiro has done is in our Udacity highway simulator,

101
00:06:10,620 --> 00:06:13,210
he's built a path planner that predicts where

102
00:06:13,210 --> 00:06:16,600
the other vehicles on the road are going to go and then figures out

103
00:06:16,600 --> 00:06:19,480
what maneuver his vehicle should take in response

104
00:06:19,480 --> 00:06:22,750
and then finally builds a series of waypoints,

105
00:06:22,750 --> 00:06:25,150
those are those green pearls you see in the video,

106
00:06:25,150 --> 00:06:27,900
for the car to drive through that's the trajectory the car should

107
00:06:27,900 --> 00:06:31,200
follow and you see when his vehicle comes up on other traffic,

108
00:06:31,200 --> 00:06:34,155
it has to figure out should it slow down and stay on its

109
00:06:34,155 --> 00:06:37,500
or should it shift right or shift left and this

110
00:06:37,500 --> 00:06:40,425
is in fact the type of decision that real self-driving cars have to make

111
00:06:40,425 --> 00:06:44,640
all the time subject to constraints like speed limits and acceleration limits so

112
00:06:44,640 --> 00:06:46,710
the ride is comfortable and you can imagine this

113
00:06:46,710 --> 00:06:48,810
gets much more complicated in urban driving

114
00:06:48,810 --> 00:06:52,980
where there are a lot more intersections and different maneuvers you can make,

115
00:06:52,980 --> 00:06:56,010
but in principle the decisions are all the same.

116
00:06:56,010 --> 00:07:00,690
So, once we've figured out what the path we want to take

117
00:07:00,690 --> 00:07:02,610
through this world is and we know where we are in

118
00:07:02,610 --> 00:07:04,620
the world and we know what the world looks like,

119
00:07:04,620 --> 00:07:07,810
the final step in the pipeline is control,

120
00:07:07,810 --> 00:07:12,570
and control is how we actually turn the steering wheel and hit the throttle

121
00:07:12,570 --> 00:07:18,190
and hit the brake in order to execute that trajectory that we built during path planning.

122
00:07:18,190 --> 00:07:21,735
So, this is one of our students, Emmy Lowe,

123
00:07:21,735 --> 00:07:25,570
this is Udacity's racetrack simulator and she has a yellow line

124
00:07:25,570 --> 00:07:29,440
here that's the ideal trajectory that her car should follow and what you

125
00:07:29,440 --> 00:07:32,680
see is there's this green line layered on top of the yellow line and

126
00:07:32,680 --> 00:07:35,920
the green line is the trajectory that her vehicle predicts it's going to

127
00:07:35,920 --> 00:07:40,150
follow based on the steering wheel and throttle and brake commands that it's sending

128
00:07:40,150 --> 00:07:45,580
and what we'd love is for that green line to be 100% aligned with the yellow line,

129
00:07:45,580 --> 00:07:49,320
but of course that's really difficult and as a human driver you probably know this.

130
00:07:49,320 --> 00:07:52,315
It's really difficult particularly on tight turns to follow

131
00:07:52,315 --> 00:07:55,719
the exact line through the middle of the lane that you'd like to follow,

132
00:07:55,719 --> 00:07:57,290
but you do pretty well with it,

133
00:07:57,290 --> 00:07:59,320
you manage to drive, and in fact computers are really,

134
00:07:59,320 --> 00:08:02,740
really good at this and the distance between the trajectory they'd like

135
00:08:02,740 --> 00:08:07,230
to follow and the trajectory they actually follow is really, really close.

136
00:08:07,230 --> 00:08:11,410
So, that's how self-driving cars work at a high level.

137
00:08:11,410 --> 00:08:14,500
We use computer vision and sensor fusion to

138
00:08:14,500 --> 00:08:18,000
get this rich picture of where we are in the world,

139
00:08:18,000 --> 00:08:24,070
we use localization to nail down precisely our location in that rich world,

140
00:08:24,070 --> 00:08:26,680
we use path planning to chart

141
00:08:26,680 --> 00:08:30,250
the course through the world to get us where we'd like to go and we use

142
00:08:30,250 --> 00:08:33,580
control to turn the steering wheel and hit the throttle and hit

143
00:08:33,580 --> 00:08:37,405
the brake to execute that trajectory and ultimately move the vehicle.

144
00:08:37,405 --> 00:08:40,240
Everything else self-driving cars do are essentially

145
00:08:40,240 --> 00:08:42,205
more sophisticated implementations of

146
00:08:42,205 --> 00:08:45,865
these key functions and this is how self-driving cars work.

147
00:08:45,865 --> 00:08:49,195
It is a really exciting time to work on self-driving cars.

148
00:08:49,195 --> 00:08:51,850
Computers are getting better and better at these tasks all of

149
00:08:51,850 --> 00:08:55,600
the time and I hope you see self-driving cars on the road near you soon.

150
00:08:55,600 --> 00:08:57,140
My name is David Silver,

151
00:08:57,140 --> 00:09:01,180
I teach Self-Driving Cars at Udacity. Thank you very much.

